[
  {
    "type": "text",
    "content": "Back in February, weobservedthe growing presence of Chinese companies on X to spread awareness of their models as part of a concerted effort to grow their market share in the U.S. For example, in the last couple months, Nathan has gotten DMs from 3 of the leading Chinese frontier model laboratories on Twitter asking to collaborate or promote their work (and zero from Western companies).",
    "audio": "./output/audio/chunk_000.wav"
  },
  {
    "type": "text",
    "content": "This has only continued \u2014 even small subdivisions of Alibaba likeTongyiare growing their presence. Another direction is how Qwen has launched a new page for tinkering with their models onQwen.ai, which feels similar in functionality toGoogle's AI Studio, as a landing page for those building with Qwen.",
    "audio": "./output/audio/chunk_001.wav"
  },
  {
    "type": "text",
    "content": "The recent Kimi K2 launch is a case study of this \u2014 days before the launch, the Kimi account messaged several people in the AI space, even smaller accounts, and offered them pre-release access to the model. This isn\u2019t new for Western companies, but is noteworthy as it becomes standard practice internationally. The way Kimi cleverly captured Western interest is by being the only provider (to our knowledge) to offer anAnthropic-compatible API.Scriptsto use K2 in Claude Code quickly emerged. With the capabilities of the model, this yielded immediateadoptionand praise onsocial media.",
    "audio": "./output/audio/chunk_002.wav"
  },
  {
    "type": "text",
    "content": "Zooming out, this is part of a larger trend where thequality of the open artifacts we are covering are maturing rapidly. In terms of overall quality, this issue ofArtifacts Logis the most impressive yet, and this extends far beyond text-only models. A year ago, it felt like a mix of half-baked research artifacts and interesting ideas. Today, there are viable open models for many real-world tasks.",
    "audio": "./output/audio/chunk_003.wav"
  },
  {
    "type": "text",
    "content": "In thelast issue, we introduced more metadata for this series, highlighting the base models used to create the artifacts we highlight. We extended our database to include the rest of our issues, where you can see the original dominance of Meta\u2019s Llama in Artifacts Log, where now Qwen has been the default for many months.",
    "audio": "./output/audio/chunk_004.wav"
  },
  {
    "type": "image",
    "path": "./output/images/img_000.jpg"
  },
  {
    "type": "text",
    "content": "More analysis like this soon, but onto the post!",
    "audio": "./output/audio/chunk_005.wav"
  },
  {
    "type": "text",
    "content": "Share",
    "audio": "./output/audio/chunk_006.wav"
  },
  {
    "type": "text",
    "content": "Qwen3-235B-A22B-Instruct-2507byQwen(released earlier today): While the dual-thinking mode wasn't introduced by Qwen, they helped popularize it with a simple mode switch by including either/no_thinkor/thinkin the prompt. To the surprise of some, Qwen is abandoning the concept aftertalking to the communityandreleasedan update to the big (now non-reasoning) MoE model. The scores are impressive (beating the Kimi K2 modelwe recently hyped as a major release), including41.8on ARC-AGI.It turns out training a hybrid reasoning model is more challenging technically than it is worth relative to the upside of downstream serving (where training two separate models, one thinker one not, is much easier).The best part of this release is that it has come with multiple reports ofstrong vibe tests. Historically, Qwen has been known to be among the benchmark-maximizing labs \u2014 thereare a fewpapersthat have come out recently highlighting signs of data contamination in Qwen base models \u2014 but the Qwen models are improving in the robustness of normal testing. We\u2019ve written multiple times on Interconnects about how labs will first shoot for strong benchmarks to get on the map, and then move to models that are more precisely those that people want to use. Quoting from ourQwen 3 post:",
    "audio": "./output/audio/chunk_007.wav"
  },
  {
    "type": "text",
    "content": "\u201dWe'll start to see if Qwen has taste/vibes. They have the benchmarks complete, and now we'll see how they compare to the likes of R1, o3, and Gemini 2.5 Pro for staying power at the frontier.\u201dQwen team members mentioned a new flagship thinking model ison the wayandjokedabout coding models coming soon. The evaluation scores are below relative to other models without a <think> section. Again, as we mentioned in ourKimi K2 post, these models are trained extensively with reinforcement learning still, but the goals of the model are more constrained. These instruct, non-thinking, models are best for when the user wants a fast time-to-first token or other automation tasks.The evaluation summary is here:",
    "audio": "./output/audio/chunk_008.wav"
  },
  {
    "type": "image",
    "path": "./output/images/img_001.jpg"
  },
  {
    "type": "text",
    "content": "SmolLM3-3BbyHuggingFaceTB: HuggingFace has released a new version of their SmolLM series alongside a very detailedwriteupof all the decisions made, including details about all the used datasets. Similar to other models, it supports a thinking and non-thinking mode. We played around with it and were impressed by the quality, it really is a model on the level of the small Qwen3 models and comes with all artifacts released \u2014 one of the few \u201copen-source\u201d models today.",
    "audio": "./output/audio/chunk_009.wav"
  },
  {
    "type": "image",
    "path": "./output/images/img_002.jpg"
  },
  {
    "type": "text",
    "content": "Kimi-K2-Instructbymoonshotai: It is hard to understate the impact of the model, which we'vecoveredalready.The new update is that they released atechnical reporttoday,with a bunch of nice methods, but nothing incredibly surprising. K2 has proven itself to be a capable model on various,unusualbenchmarks, matching Opus onLMArenawhile becoming one of the most used models onOpenRouter.",
    "audio": "./output/audio/chunk_010.wav"
  },
  {
    "type": "text",
    "content": "FLUX.1-Kontext-devbyblack-forest-labs: After the GPT-4o image release, a lot of people (especially on social media) claimed that omni models are the future and that specialized image models will be unable to catch up. Avid readers of the Artifacts series know that this is not the case, seeStep1X Editas an example. BFL has now released their editing model based onFLUX.1-Kontext-dev. Fun fact: FLUX.1-Kontext-dev is the model with the most fine-tunes / adapters on HuggingFace, despite its non-commercial license.",
    "audio": "./output/audio/chunk_011.wav"
  },
  {
    "type": "image",
    "path": "./output/images/img_003.jpg"
  },
  {
    "type": "text",
    "content": "Hunyuan-A13B-Instructbytencent: Tencent releases both a 7B dense and a 80B total / 13B active MoE model. It also features 256K context, has very solid benchmark scores (including function calling). Aside from that, people actually use it and are impressed by it! We sound like a broken record, but Chinese labs and companies continue to outbid each other in the open model space with very solid models.Edit:Unfortunately this model is up there in terms ofbad licenses. A portion of it states: \u201cYou must not use, reproduce, modify, distribute, or display the Tencent Hunyuan Works, Output or results of the Tencent Hunyuan Works outside the Territory. Any such use outside the Territory is unlicensed and unauthorized under this Agreement.\u201d Here, theterritoryis everywhere but the EU, UK, and SK.",
    "audio": "./output/audio/chunk_012.wav"
  },
  {
    "type": "text",
    "content": "ERNIE-4.5-21B-A3B-PTbybaidu: Baidu followed up on their promise to release their flagship model, ERNIE, in various sizes, both dense and MoE. The models are licensed under Apache 2.0. However, the model got mixed reactions on social media, with some people being surprised with the quality, while others are disappointed with its performance on tasks outside of usual benchmarks.",
    "audio": "./output/audio/chunk_013.wav"
  },
  {
    "type": "text",
    "content": "pangu-pro-moe-modelbyIntervitensInc: A MoE trained by the Huawei Pangu team on Ascend NPUs. However, the model release is overshadowed byallegationsof them being upcycled DeepSeek / Qwen models.",
    "audio": "./output/audio/chunk_014.wav"
  },
  {
    "type": "text",
    "content": "EXAONE-4.0-32BbyLGAI-EXAONE: LG also continues to release new models. This iteration adds a dual-thinking mode, a 3:1 local:global attention and a focus on tool calling. However, it is released under a noncommercial license.",
    "audio": "./output/audio/chunk_015.wav"
  },
  {
    "type": "text",
    "content": "Apriel-Nemotron-15b-ThinkerbyServiceNow-AI: ServiceNow is also joining the ever-growing list of companies training their own reasoning models.",
    "audio": "./output/audio/chunk_016.wav"
  },
  {
    "type": "text",
    "content": "micro-g3.3-8b-instruct-1bbyibm-ai-platform: A 1B model building upon the 8B Granite model, featuring only 3 hidden layers.",
    "audio": "./output/audio/chunk_017.wav"
  },
  {
    "type": "text",
    "content": "DeepSeek-TNG-R1T2-Chimerabytngtech: The German TNG has thrown the new R1 into the model soup (because DeepSeek released an updated R1 model onMay 28th), improving performance over the original Chimera model even further.",
    "audio": "./output/audio/chunk_018.wav"
  },
  {
    "type": "image",
    "path": "./output/images/img_004.jpg"
  },
  {
    "type": "text",
    "content": "AI21-Jamba-Large-1.7byai21labs: An update to the hybrid SSM-Transformer model series from AI21.",
    "audio": "./output/audio/chunk_019.wav"
  },
  {
    "type": "text",
    "content": "FlexOlmo-7x7B-1Tbyallenai: A new model by Ai2, wheredifferent organizations can train experts on their data to improve a shared model. Theblogprovides more details about the model and its training process.",
    "audio": "./output/audio/chunk_020.wav"
  },
  {
    "type": "text",
    "content": "Phi-4-mini-flash-reasoningbymicrosoft: A hybrid model to speed up inference.",
    "audio": "./output/audio/chunk_021.wav"
  },
  {
    "type": "text",
    "content": "OctoThinker-3B-Hybrid-ZerobyOctoThinker: The OctoThinker team has released a comprehensivepaperabout their mid-training process.",
    "audio": "./output/audio/chunk_022.wav"
  },
  {
    "type": "text",
    "content": "OpenReasoning-Nemotron-32Bbynvidia: A new version of NVIDIA's reasoning model, using the same prompts as theprevious version, but with traces generated by R1 0528.",
    "audio": "./output/audio/chunk_023.wav"
  },
  {
    "type": "text",
    "content": "Ovis-U1-3BbyAIDC-AI: Another omni model by another subdivision of Alibaba.",
    "audio": "./output/audio/chunk_024.wav"
  },
  {
    "type": "text",
    "content": "GLM-4.1V-9B-ThinkingbyTHUDM: An extension of zAI's GLM-9B-0414 to also support images as inputs. This is one of the longer-tail of very strong open weight model laboratories from China.",
    "audio": "./output/audio/chunk_025.wav"
  },
  {
    "type": "image",
    "path": "./output/images/img_005.jpg"
  }
]